---
title: 大模型相关
categories:
  - Note
  - AI
  - NLP
abbrlink: 8159
date: 2023-10-05 11:47:18
---

## 大模型微调范式

* Pre-training （成本约模型参数的4倍）
    * 全量训练，无标注数据
* Fine-tuning（成本约模型参数的4倍）
    * 全量训练，标注数据
* Parameter efficient tuning （成本与部署成本相近）
    * Adapter：加入adapter层训练，会引入额外的推理开销；
    * Prompt/Prefix tuning：效果更优，需要调参；
    * Lora：依靠权重的低秩分解特点，没有额外推理开销

## 大模型 + 本地知识库

* LangChain

* Text Splitter

* Embedding 

    * 语义相似度：在nlp领域里，一般使用cosine相似度作为语义相似度的度量，评估两个向量在语义空间上的分布状况
        $$
        cosine(v, w)
        = \frac{v\;\cdot\;w}{\abs{v}\abs{w}}
        = \frac{\sum\limits_{i=1}^Nv_iw_i}{\sqrt{\sum\limits_{i=1}^Nv_i^2}\sqrt{\sum\limits_{i=1}^Nw_i^2}}
        $$
        
    * word2vec
    
* 效果优化方向：模型微调（llm和embedding）专业词汇识别、文档加工（在文本分段后，对每段分别进行总结，基于总结内容语义进行匹配）、借助不同的模型能力（在text2sql、text2cpyher场景下需要产生代码时，可借助不同的模型能力）
