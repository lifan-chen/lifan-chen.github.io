<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>VisCPM</title>
      <link href="/2023/09/14/VisCPM/"/>
      <url>/2023/09/14/VisCPM/</url>
      
        <content type="html"><![CDATA[<h2 id="title">1 Title</h2><p>Large Multilingual Models Pivot Zero-Shot Multimodal Learning acrossLanguages</p><h2 id="abs">2 abs</h2><ul><li><p>MpM, an effective training paradigm for training large multimodal(多模态) models in low-resource language</p><p>MPM demonstrates that Multilingual language models can Pivotzero-shot Multimodal learning across languages</p><p>pretrained on English-only image-text data can well generalize toother languages in a zero-shot manner</p></li><li><p>open-source codes and model weights athttps://github.com/OpenBMB/VisCPM</p></li></ul><h2 id="intro">3 Intro</h2><ul><li>the multimodal generative capabilities across images and text can bedivided into two categories<ul><li>In the field of image-totext generation, multimodal large languagemodels (like GPT-4 [38], LLaVA [32] and InstructBLIP [13]) exhibitremarkable multimodal conversational and reasoning abilities(多模态对话和推理能力) based on images</li><li>In the field of text-to-image generation, models such as Imagen [45]and Stable Diffusion [43] excel in generating highly realistic andrelevant images (高度逼真和相关的图像) based on text prompts</li></ul></li><li>the paucity of multimodal data resources in non-English languages,the progress of multimodal research in these languages remains hindered<ul><li>To address this challenge, we propose MPM, an effective trainingparadigm for large multimodal models in non-English languages</li><li>multilingual learners can effectively align the visual semanticswith newly acquired language based on established multimodal andmultilingual alignment</li><li>MPM divides the non-English multimodal learning into two consecutivestages: ==multilingual alignment and multimodal alignment==. The formerfocuses on building a multilingual model, while the latter culminates ina multimodal model spanning multiple languages.</li></ul></li><li>for multilingual alignment, MPM harnesses a pretrained multilinguallarge language model (LLM) as the backbone language model</li><li>for the multimodal alignment, MPM trains the visual modules based onthe multilingual model exclusively on English image-text pairs to alignEnglish and visual semantics.</li></ul><h2 id="related-work">4 Related Work</h2><ul><li><p>Image-to-text Models</p><ul><li><p>Traditional image-to-text generation models mainly focus on thetask of image caption and visual question answering.</p></li><li><p>Recently, the mainstream of image-to-text has turned tomultimodal LLM, focus on rendering LLM capable of interaction with users(LLM能够与用户进行多模态交互).</p><ul><li><p>connect the visual module and LLM with perceivers</p><p>VPGTrans [64] explores the transferability of visual modules acrossLLM. 探索视觉模块跨LLM的可转移性</p><p>LLaVA [32] and Mini-GPT-4 [68] build visual content-related dialog bytransferring image captions into conversation data using GPT-4.通过使用GPT-4将图像描述转换为对话数据来构建与视觉内容相关的对话</p><p>InstructBLIP [13] and M<sup>3</sup>IT [30] incorporate downstreamvision-language datasets to construct instruction data.结合了下游的视觉语言数据集来构建指令数据</p></li></ul></li></ul></li><li><p>Text-to-image Models</p><ul><li>In the early stages: generative adversarial networks andauto-regressive generation.</li><li>Recently, large-scale diffusion-based text-to-image models such asDALLE-2, Imagen, and Stable Diffusion have taken center stage.</li></ul></li><li><p>Multilingual Multimodal Models</p><ul><li><p>the extension of multimodal models to include multilingualcapabilities has become a key research focus over the post fewyears.</p></li><li><p>make efforts to extend the powerful image-text model CLIP tohandle more languages using techniques such as ==knowledge distillation[5]== or ==contrastive learning [4, 10, 25]==</p></li><li><p>Other studies have aimed to create a universal framework formultilingual vision-language pretraining.</p></li><li><p>Differing from these studies, which try to simultaneously achievemultilingual and multimodal alignment, we focus on effectivelyleveraging pretrained multilingual LLMs in multimodal learning acrossvarious languages.</p><ul><li><p>cross-lingual transfer from multilingual LLM in multimodalsetting</p><p>Ying-VLM shows that instruction tuning in English can generalize toother languages.</p><p>MultiFusion discover that the multilingual language model can helpcross-lingual transfer in text-to-image generation.</p></li></ul></li><li><p>Differently, our proposed MPM provides a more systematicalformulation for the training of multilingual multimodal models anddemonstrates that the zero-shot transfer performance of these models cansurpass that of models trained on native-language multimodaldata.</p></li></ul></li></ul><h2 id="method">5 Method</h2><h3 id="mpm-training-paradigm">5.1 MPM Training Paradigm</h3><ul><li><p>Multimodal learning can be formulated as modeling therelationship between images, denoted as <spanclass="math inline">\(x\)</span>, and text, denoted as <spanclass="math inline">\(y\)</span>, in a target language <spanclass="math inline">\(l_t\)</span>.</p><p>we introduce the privot language <spanclass="math inline">\(l_p\)</span>, which contains abundant multimodalpairs <span class="math inline">\(D_p = \{(x_i, y_i^{l_p})\}^M_{i =1}\)</span>, where <span class="math inline">\(M \gg N\)</span>. (<spanclass="math inline">\(N\)</span> is the number of <spanclass="math inline">\(l_t\)</span>'s pairs).</p><p>Imitating the human learning mechanism that can naturally alignvisual concepts with various learned languages, MPM aims to transfervisual concepts learned in the pivot language to the targetlanguage.</p></li><li><p>MPM divides the multimodal learning process in target language<span class="math inline">\(l_t\)</span> into two consecutive stages</p><ul><li><p>multilingual alignment</p><p>For this, MPM aims to establish the cross-lingual alignment for <spanclass="math inline">\(l_t\)</span> and <spanclass="math inline">\(l_p\)</span>. This is achieved by directlyleveraging a pretrained multilingual LLM, denoted as <spanclass="math inline">\(f_\alpha\)</span>, which can provide close hiddenrepresentations for text pair <spanclass="math inline">\(y^{l_t}\)</span> and <spanclass="math inline">\(y^{l_p}\)</span> with similar semantics. i.e.,<span class="math inline">\(f_\alpha(y^{l_t}) \approxf_\alpha(y^{l_p})\)</span></p></li><li><p>multimodal alignment</p><p>For this, MPM utilize the sufficient multimodal resource <spanclass="math inline">\(D_p\)</span> in the pivot language and optimizethe image-to-text objective <spanclass="math inline">\(p_\theta(y^{l_p}|x)\)</span> and text-to-imageobjective <spanclass="math inline">\(p_\phi(x|y^{l_p})\)</span>.</p></li></ul></li><li><p>In the follow sections, we introduce the training process of<strong>multimodal alignment</strong> stage</p><p>It's worth noting that MPM is agnostic to the specific modelarchitecture and training method (对具体的模型架构和训练方法并不敏感),which enables us to flexibly utilize existing highly effective modelarchitectures and training techniques in each task(这使得我们可以在每个任务中灵活地利用现有的高效模型结构和训练技术).</p></li></ul><h4 id="the-image-to-text-generation">5.1.1 the image-to-textgeneration</h4><ul><li><p>can be roughly summarized as generating description for inputimages, is to learn the conditional distribution <spanclass="math inline">\(p_{\theta}(y^{l_t}|x)\)</span> parameterized by<span class="math inline">\(\theta\)</span></p></li><li><p>we incorporate an image encoder module <spanclass="math inline">\(h_\xi\)</span> parameterized by <spanclass="math inline">\(\xi\)</span> (以 <spanclass="math inline">\(\xi\)</span> 参数化的图像编码器模块 <spanclass="math inline">\(h_\xi\)</span>) to provide visual feature <spanclass="math inline">\(z = h_xi(x)\)</span>. These visual features<strong>z</strong> are then concatenated with the text embedding(将这些视觉特征z与文本嵌入进行拼接) as input the multilingualLLM.</p></li><li><p>MPM's training process for image-to-text generation consists oftwo sub-stages</p><ul><li><p>Multimodal Pretraining</p><p>pretrain the visual module to align it with LLM on a large scale ofimage-text pairs using the language modeling objective: <spanclass="math display">\[  \mathcal L_1(p_\theta, \mathcal D_p) = - \sum_{i=1}^{M}log \, p_\theta(y_i^{l_p}|h_\xi(x_i))  \]</span> fix the parameters of LLM <spanclass="math inline">\((\theta = \{\xi\})\)</span> (固定LLM的参数) toprevent the powerful capabilities of LLM from being influenced by shorttexts in the image-text pairs.</p></li><li><p>Instruction Tuning</p><p>To enhance models' capabilities in following human instructions, weconduct instruction tuning on elaborately curated ==multimodalinstruction tuning datasets== built by blending the existing multimodalinstruction tuning datasets in the pivot language and their translatedversion in the target language.</p><p>Both the visual module and multilingual LLM are fine-tuned, i.e.,<span class="math inline">\(\theta = \{\xi, \alpha\}\)</span>, bymaximizing the probability of the response.</p><p>we find a ==<em>quasi-zero-shot</em>== transfer capability ofmultilingual multimodal models in this scenario.</p><p>If excluding the translated variant in the target language and solelyperforming instruction tuning using the pivot language, when given animage <span class="math inline">\(x\)</span> and a question or aninstruction <span class="math inline">\(y_q^{l_t}\)</span> in the targetlanguage, the resultant model responds accurately though mostly in thepivot language. This can be attributed to the close resemblance betweenthe hidden representation of instructions in two languages provided bythe multilingual LLM, i.e., <spanclass="math inline">\(f_\alpha(y_q^{l_p}) \approxf_\alpha(y_q^{l_t})\)</span> ==没看懂==</p><p>Since both pretraining and instruction tuning stages employ textcomponents solely in the pivot language, the LLM can understand thequestion in the target language but cannot calibrate the response in thesame language. ==没看懂==</p></li></ul></li></ul><h4 id="the-text-to-image-generation">5.1.2 the text-to-imagegeneration</h4><ul><li>is to synthesize relevant images given input text prompts, is tolearn <span class="math inline">\(p_{\phi}(x|y^{l_t})\)</span>parameterized by <span class="math inline">\(\phi\)</span></li></ul><h3 id="viscpm">5.2 VISCPM</h3><h2 id="exp">6 Exp</h2><h2 id="conclusion">7 Conclusion</h2><ul><li>MPM</li><li>utilize a multilingual LLM as a pivotal intermediary between visionsignals and target languages</li><li>VISCPM shows remarkable capability in Chinese image-to-text andtext-to-image tasks</li><li>by only rely on English multimodal data</li></ul>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown语法</title>
      <link href="/2023/08/31/Markdown%E8%AF%AD%E6%B3%95/"/>
      <url>/2023/08/31/Markdown%E8%AF%AD%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="数学公式">数学公式</h2><h3 id="矩阵相关">矩阵相关</h3><ol type="1"><li><p>不带括号的普通矩阵</p><p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;matrix&#125;</span><br><span class="line">   a <span class="built_in">&amp;</span> b <span class="built_in">&amp;</span> c <span class="built_in">&amp;</span> d <span class="built_in">&amp;</span> e <span class="keyword">\\</span></span><br><span class="line">   f <span class="built_in">&amp;</span> g <span class="built_in">&amp;</span> h <span class="built_in">&amp;</span> i <span class="built_in">&amp;</span> j <span class="keyword">\\</span></span><br><span class="line">   k <span class="built_in">&amp;</span> l <span class="built_in">&amp;</span> m <span class="built_in">&amp;</span> n <span class="built_in">&amp;</span> o <span class="keyword">\\</span></span><br><span class="line">   p <span class="built_in">&amp;</span> q <span class="built_in">&amp;</span> r <span class="built_in">&amp;</span> s <span class="built_in">&amp;</span> t</span><br><span class="line"><span class="keyword">\end</span>&#123;matrix&#125;</span><br></pre></td></tr></table></figure></p><p><span class="math display">\[\begin{matrix}    a &amp; b &amp; c &amp; d &amp; e \\    f &amp; g &amp; h &amp; i &amp; j \\    k &amp; l &amp; m &amp; n &amp; o \\    p &amp; q &amp; r &amp; s &amp; t   \end{matrix}\]</span></p></li><li><p>带中括号的矩阵</p><p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\left</span>[</span><br><span class="line"> <span class="keyword">\begin</span>&#123;matrix&#125;</span><br><span class="line">   a <span class="built_in">&amp;</span> b <span class="built_in">&amp;</span> c <span class="built_in">&amp;</span> d <span class="built_in">&amp;</span> e <span class="keyword">\\</span></span><br><span class="line">   f <span class="built_in">&amp;</span> g <span class="built_in">&amp;</span> h <span class="built_in">&amp;</span> i <span class="built_in">&amp;</span> j <span class="keyword">\\</span></span><br><span class="line">   k <span class="built_in">&amp;</span> l <span class="built_in">&amp;</span> m <span class="built_in">&amp;</span> n <span class="built_in">&amp;</span> o <span class="keyword">\\</span></span><br><span class="line">   p <span class="built_in">&amp;</span> q <span class="built_in">&amp;</span> r <span class="built_in">&amp;</span> s <span class="built_in">&amp;</span> t</span><br><span class="line">  <span class="keyword">\end</span>&#123;matrix&#125; </span><br><span class="line"><span class="keyword">\right</span>]</span><br></pre></td></tr></table></figure></p><p><span class="math display">\[\left[  \begin{matrix}    a &amp; b &amp; c &amp; d &amp; e \\    f &amp; g &amp; h &amp; i &amp; j \\    k &amp; l &amp; m &amp; n &amp; o \\    p &amp; q &amp; r &amp; s &amp; t   \end{matrix}\right]\]</span></p></li><li><p>带大括号的矩阵</p><p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\left</span><span class="keyword">\&#123;</span></span><br><span class="line"> <span class="keyword">\begin</span>&#123;matrix&#125;</span><br><span class="line">   a <span class="built_in">&amp;</span> b <span class="built_in">&amp;</span> c <span class="built_in">&amp;</span> d <span class="built_in">&amp;</span> e <span class="keyword">\\</span></span><br><span class="line">   f <span class="built_in">&amp;</span> g <span class="built_in">&amp;</span> h <span class="built_in">&amp;</span> i <span class="built_in">&amp;</span> j <span class="keyword">\\</span></span><br><span class="line">   k <span class="built_in">&amp;</span> l <span class="built_in">&amp;</span> m <span class="built_in">&amp;</span> n <span class="built_in">&amp;</span> o <span class="keyword">\\</span></span><br><span class="line">   p <span class="built_in">&amp;</span> q <span class="built_in">&amp;</span> r <span class="built_in">&amp;</span> s <span class="built_in">&amp;</span> t</span><br><span class="line">  <span class="keyword">\end</span>&#123;matrix&#125; </span><br><span class="line"><span class="keyword">\right</span><span class="keyword">\&#125;</span></span><br></pre></td></tr></table></figure></p><p><span class="math display">\[\left\{  \begin{matrix}    a &amp; b &amp; c &amp; d &amp; e \\    f &amp; g &amp; h &amp; i &amp; j \\    k &amp; l &amp; m &amp; n &amp; o \\    p &amp; q &amp; r &amp; s &amp; t   \end{matrix}\right\}\]</span></p></li><li><p>矩阵前加参数</p><p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A=</span><br><span class="line"><span class="keyword">\left</span><span class="keyword">\&#123;</span></span><br><span class="line"> <span class="keyword">\begin</span>&#123;matrix&#125;</span><br><span class="line">   a <span class="built_in">&amp;</span> b <span class="built_in">&amp;</span> c <span class="built_in">&amp;</span> d <span class="built_in">&amp;</span> e <span class="keyword">\\</span></span><br><span class="line">   f <span class="built_in">&amp;</span> g <span class="built_in">&amp;</span> h <span class="built_in">&amp;</span> i <span class="built_in">&amp;</span> j <span class="keyword">\\</span></span><br><span class="line">   k <span class="built_in">&amp;</span> l <span class="built_in">&amp;</span> m <span class="built_in">&amp;</span> n <span class="built_in">&amp;</span> o <span class="keyword">\\</span></span><br><span class="line">   p <span class="built_in">&amp;</span> q <span class="built_in">&amp;</span> r <span class="built_in">&amp;</span> s <span class="built_in">&amp;</span> t</span><br><span class="line">  <span class="keyword">\end</span>&#123;matrix&#125; </span><br><span class="line"><span class="keyword">\right</span><span class="keyword">\&#125;</span></span><br></pre></td></tr></table></figure></p><p><span class="math display">\[A=\left\{  \begin{matrix}    a &amp; b &amp; c &amp; d &amp; e \\    f &amp; g &amp; h &amp; i &amp; j \\    k &amp; l &amp; m &amp; n &amp; o \\    p &amp; q &amp; r &amp; s &amp; t   \end{matrix}\right\}\]</span></p></li><li><p>矩阵中间有省略号</p><p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A=</span><br><span class="line"><span class="keyword">\left</span><span class="keyword">\&#123;</span></span><br><span class="line"> <span class="keyword">\begin</span>&#123;matrix&#125;</span><br><span class="line">   a <span class="built_in">&amp;</span> b <span class="built_in">&amp;</span> <span class="keyword">\cdots</span> <span class="built_in">&amp;</span> e <span class="keyword">\\</span></span><br><span class="line">   f <span class="built_in">&amp;</span> g <span class="built_in">&amp;</span> <span class="keyword">\cdots</span> <span class="built_in">&amp;</span> j <span class="keyword">\\</span></span><br><span class="line">   <span class="keyword">\vdots</span> <span class="built_in">&amp;</span> <span class="keyword">\vdots</span> <span class="built_in">&amp;</span> <span class="keyword">\ddots</span> <span class="built_in">&amp;</span> <span class="keyword">\vdots</span> <span class="keyword">\\</span></span><br><span class="line">   p <span class="built_in">&amp;</span> q <span class="built_in">&amp;</span> <span class="keyword">\cdots</span> <span class="built_in">&amp;</span> t</span><br><span class="line">  <span class="keyword">\end</span>&#123;matrix&#125; </span><br><span class="line"><span class="keyword">\right</span><span class="keyword">\&#125;</span></span><br></pre></td></tr></table></figure></p><p><span class="math display">\[A=\left\{  \begin{matrix}    a &amp; b &amp; \cdots &amp; e \\    f &amp; g &amp; \cdots &amp; j \\    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\    p &amp; q &amp; \cdots &amp; t   \end{matrix}\right\}\]</span></p></li><li><p>矩阵中间加横线</p><p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A=</span><br><span class="line"><span class="keyword">\left</span><span class="keyword">\&#123;</span></span><br><span class="line"> <span class="keyword">\begin</span>&#123;array&#125;&#123;cccc|c&#125;</span><br><span class="line">     a <span class="built_in">&amp;</span> b <span class="built_in">&amp;</span> c <span class="built_in">&amp;</span> d <span class="built_in">&amp;</span> e <span class="keyword">\\</span></span><br><span class="line">     f <span class="built_in">&amp;</span> g <span class="built_in">&amp;</span> h <span class="built_in">&amp;</span> i <span class="built_in">&amp;</span> j <span class="keyword">\\</span></span><br><span class="line">     k <span class="built_in">&amp;</span> l <span class="built_in">&amp;</span> m <span class="built_in">&amp;</span> n <span class="built_in">&amp;</span> o <span class="keyword">\\</span></span><br><span class="line">     p <span class="built_in">&amp;</span> q <span class="built_in">&amp;</span> r <span class="built_in">&amp;</span> s <span class="built_in">&amp;</span> t</span><br><span class="line">  <span class="keyword">\end</span>&#123;array&#125; </span><br><span class="line"><span class="keyword">\right</span><span class="keyword">\&#125;</span></span><br></pre></td></tr></table></figure></p><p><span class="math display">\[A=\left\{  \begin{array}{cccc|c}      a &amp; b &amp; c &amp; d &amp; e \\      f &amp; g &amp; h &amp; i &amp; j \\      k &amp; l &amp; m &amp; n &amp; o \\      p &amp; q &amp; r &amp; s &amp; t   \end{array}\right\}\]</span></p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> remark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>影像生成模型</title>
      <link href="/2023/08/31/%E5%BD%B1%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
      <url>/2023/08/31/%E5%BD%B1%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="diffusion-model">Diffusion Model</h2><h3 id="生词">1 生词</h3><p>latent：adj. 潜在的；潜伏的；隐藏的； n. 潜指印；</p><p>decompose v. 分解；(使)还原；(使)腐烂；衰变；</p><p>state-of-the-art adj. 最先进的</p><p>fidelity：n. 保真度</p><p>hierarchy n. 层次体系；等级制度（尤指社会或组织）；统治集团；</p><p>exploit v. 开发；利用；剥削；利用…谋私利； n. 功绩；</p><p>prone adj.易于遭受；有做（坏事）的倾向；有做…倾向的；易于遭受…的；</p><p>excessive adj. 过分的；过度的； 过多的；额外；极度的；</p><p>imperceptible adj. （小得）无法察觉的；感觉不到的；觉察不到的；细微的；</p><p>distortion：n. 扭曲；歪曲；【电】(信号,波形等的)失真；畸变；变形；失真度；</p><p>trade-off：权衡</p><p>perceptual adj. 知觉的；感知的</p><p>superfluous adj. 过剩的；过多的；多余的；</p><p>manifold n. 管汇；汇集；复写本；【机械工程】歧管； adj.许多的；多样的；由许多部分形成的；繁茂的； v. 复印；</p><p>bluriness n. 模糊性</p><p>quantization n. 〔物〕量子化；分层； 网络释义：量化；量化程式；量化运算</p><p>autoregressively 自回归</p><p>probabilistic 随机</p><p>discrete 离散的</p><p>modalities 模态词</p><p>intermediate representation 中间表示</p><p>tractable 易加工的</p><p>stagnate 停滞不前</p><p>spectacular adj. 壮观的；壮丽的；令人惊叹的； n.壮观的场面；精彩的表演</p><h3 id="论文结构">2 论文结构</h3><h4 id="title">1. Title</h4><p>High-Resolution Image Synthesis with Latent Diffusion Models</p><h4 id="abs">2. Abs</h4><ul><li>Diffusion models（DMs）achieve state-of-the-art synthesis results<ul><li>image data and beyond</li><li>allow for a guiding mechanism to control the image generationprocess <strong>without retraining</strong></li><li>operate directly in pixel space, consume hundreds of GPU days andinference is expensive due to <strong>sequentialevaluation</strong>s</li></ul></li><li>To enable DM training on limited computational resources whileretaining quality and flexibility<ul><li>apply DM in the latent space of <strong>pretrainedautoencoders</strong></li></ul></li><li>In contrast to previous work<ul><li>the first time to reach a near-optimal point (between complexityreduction and detail preservation)</li></ul></li><li>Introduce <strong>cross-attention layers</strong> into the modelarchitecture<ul><li>turn DM into generators (inputs such as text or bounding boxes)</li><li>high-resolution synthesis in a convolutional manner</li></ul></li><li>Latent diffusion models (LDMs)<ul><li>achieve new state of art scores for image inpainting andclass-conditional image synthesis</li><li>unconditional image generation, text-to-image synthesis, and superresolution</li><li>significantly reducing computational requirement compared topixel-based DMs</li></ul></li></ul><h4 id="intro">3. Intro</h4><ul><li><p>Images synthesis with the greatest computational demands</p></li><li><p>high-resolution synthesis of complex, natural scenes is presentlydominated by scaling up likelihood-based models containing billions ofparameters in ==autoregressive (AR) transformers==</p></li><li><p>DMs' application:</p><ul><li>class-conditional: image synthesis, super-resolution</li><li>unconditional: inpainting, colorization, stroke-based synthesis</li></ul></li><li><p>Being likelihood-based models, they ==do not exhibitmode-collapse and training instabilities== as GANs and, by heavilyexploiting ==parameter sharing==, they can model highly complexdistributions of natural images without involving billions of parametersas in AR models</p></li><li><p>Democratizing High-Resolution Image Synthesis</p><p>DMs spends excessive amounts of capacity</p><p>​ train and evaluate such a model requires repeated functionevaluations (and gradient computions) in the high-dimensional space ofRGB images</p></li><li><p>Departure to Latent Space</p></li></ul><h4 id="related-work">4. Related Work</h4><h4 id="method">5. Method</h4><ul><li><p>DMs allow to ignore perceptually irrelevant details byundersampling the corresponding loss terms ==[29]==???</p></li><li><p>Introduce an explicit separation of the compressive from thegenerative learning phase</p><ul><li>use an autoencoding model (learn a space)</li><li>offer reduced computational complexity</li></ul></li><li><p>Several advantages</p></li><li><p>Perceptual Image Compression</p><ul><li>==based on previous work [23]==</li><li>an autoencoder trained by combination of a perceptual loss [102] anda patch-based adversarial objective</li><li>ensure that<ul><li>the reconstructions are confined to the image manifold by enforcinglocal realism</li><li>avoid bluriness introduced by relying solely on pixel-spacelosses</li></ul></li><li>in order to avoid high-variance latent spaces, experiment with twodifferent kinds of regularizations<ul><li>The first variant, KL-reg., imposes a slight KL-penalty towards astandard normal on the learned latent, similar to a VAE<ul><li>VQ-reg. uses a vector quantization layer ==[93]== within thedecoder</li></ul></li><li>rely on an arbitrary 1D oderding of the learned space<em>z</em></li></ul></li></ul></li><li><p>Latent Diffusion Models</p><ul><li>Diffusion Models<ul><li>probabilistic</li><li>designed to learn a data distribution by gradually denoising anormally distributed variable</li><li>trained to predict a denoised variant of their input <spanclass="math inline">\(x_t\)</span>, where <spanclass="math inline">\(x_t\)</span> is a noisy version of the input <spanclass="math inline">\(x\)</span></li></ul></li><li>Generative Modeling of Latent Represenetations (潜在表征的生成建模)<ul><li>have an efficient, low-dimensional latent space in whichhigh-frequency. Compared to the high-dimensional pixel space, this spaceis more suitable for likehood-based generative models<ul><li>focus on the important, semantic bits of the data</li><li>train in a lower dimensional, computationally much more efficientspace</li></ul></li><li>attention-based transformer model<ul><li>in a highly compressed, discrete latent space</li><li>take advantage of image-specific inductive biases<ul><li>build the underlying UNet primarily from 2D convolutationallayers</li><li>focus on the objective on the perceptually most relevant bits usingthe reweighted bound</li></ul></li></ul></li></ul></li></ul></li><li><p>Conditioning Mechanisms</p><ul><li><p>diffusion models can</p><ul><li>be implemented with a conditional denoising autoencoder</li><li>paves the way to controlling the synthesis process through inputs<span class="math inline">\(y\)</span> such as <strong>text, semanticmaps or other image-to-image translation tasks</strong>.</li></ul></li><li><p>we turn DMs into more flexible conditional image generators byaugmentimg their underlying UNet backbone with the cross-attentionmechanism</p><p>我们通过交叉注意力机制增强DM的底层UNet主干，使其成为更灵活的条件图像生成器</p></li><li><p>To pre-process <span class="math inline">\(y\)</span> fromvarious modalities (such as language prompts)</p><ul><li>introduce a domain specific encoder that projects <spanclass="math inline">\(y\)</span> to an intermediate representation,which is then mapped to the intermediate layers of the UNet via across-attention layer implementing Attention ?????</li></ul></li></ul></li></ul><h4 id="exp">6. Exp</h4><p>LDMs provide means to flexible and computationally tractablediffusion based image synthesis also including high-resolutiongeneration of various image modalities.</p><ul><li><p>analyze the gains of our models comapred to pixel-based diffusionmodels in both training and inference</p><ul><li>LDMs trained in VQ-regularized latent spaces achieve better samplequality</li></ul></li><li><p>On Perceptual Compression Tradeoffs</p><p>analyzes the behavior of our LDMs with different downsampling factors<span class="math inline">\(f ∈ \{1, 2, 4, 8, 16, 32\}\)</span></p><p>a single NVIDIA A100</p><p>train all models for the same number of steps and with the samenumber of parameters</p><ul><li>small downsampling factors for LDM-{1, 2} result in slow trainingprocess</li><li>overly large values of f cause stagnating fidelity after comparablyfew training steps</li><li>we attribute this to<ul><li>leaving most of perceptual compression to the diffusion model</li><li>too strong first stage compression result in information loss andthus limiting the achievable quality.</li></ul></li><li>LDM-{4-16} strike a good balance between efficiency and perceptuallyfaithful result, which manifests in a significant ==FID[28]== gap of 28between pixel-based diffusion(LDM-1) and LDM-8 after 2M trainingsteps.</li></ul><p>==LDM-4 and LDM-8== lie in the best behaved regime for achievinghigh-quality synthesis result.</p></li><li><p>Image Generation with Latent Diffusion</p><ul><li>FID and ==Precision-and-Recall[49]==</li></ul></li><li><p>Conditional Latent Diffusion</p><ul><li><p>Transformer Encoders for LDMs</p><ul><li>text-to-image image model<ul><li>train a 1.45B paramter model conditioned on language prompts onLAION-400M</li><li>employ the BERT-tokenizer and implement <spanclass="math inline">\(\tau_\theta\)</span> as a transformer to infer alatent code ==which is mapped into UNet via cross-attention==</li></ul></li><li>to further analyze the flexibility of the cross-attention basedconditioning mechanism<ul><li>train models to synthesize images based on semantic layouts onOpenImages [48], and finetune on COCO [4]</li></ul></li><li>our best-performing class-conditional ImageNet models with <spanclass="math inline">\(f ∈ \{4, 8\}\)</span> outperform the state of theart diffusion model ADM while significantly reducing computationalrequirements and parameter count</li></ul></li><li><p>Convolutional Sampling Beyond <spanclass="math inline">\(256^2\)</span></p><p>semantic synthesis, super-resolution and inpainting</p></li></ul></li><li><p>Super-Resolution with Latent Diffusion</p><p>等待阅读</p></li><li><p>Inpainting with Latent Diffusion</p><p>等待阅读</p></li></ul><h4 id="conclusion">7. Conclusion</h4><p>improve both the training and sampling efficiency</p><p>without task-specific architectures</p><h2 id="generative-adversarial-network-gan">Generative AdversarialNetwork (GAN)</h2>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【AI夏令营】生命科学赛道-Baseline精读</title>
      <link href="/2023/08/22/%E3%80%90AI%E5%A4%8F%E4%BB%A4%E8%90%A5%E3%80%91%E7%94%9F%E5%91%BD%E7%A7%91%E5%AD%A6%E8%B5%9B%E9%81%93-Baseline%E7%B2%BE%E8%AF%BB/"/>
      <url>/2023/08/22/%E3%80%90AI%E5%A4%8F%E4%BB%A4%E8%90%A5%E3%80%91%E7%94%9F%E5%91%BD%E7%A7%91%E5%AD%A6%E8%B5%9B%E9%81%93-Baseline%E7%B2%BE%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>pip常用命令</title>
      <link href="/2023/08/18/pip%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2023/08/18/pip%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="使用代理">1 使用代理</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">临时设置</span></span><br><span class="line">pip install -r requirements.txt --proxy=代理服务器IP:端口号</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> remark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【AI夏令营】生命科学赛道-Baseline</title>
      <link href="/2023/08/17/%E3%80%90AI%E5%A4%8F%E4%BB%A4%E8%90%A5%E3%80%91%E7%94%9F%E5%91%BD%E7%A7%91%E5%AD%A6%E8%B5%9B%E9%81%93-Baseline/"/>
      <url>/2023/08/17/%E3%80%90AI%E5%A4%8F%E4%BB%A4%E8%90%A5%E3%80%91%E7%94%9F%E5%91%BD%E7%A7%91%E5%AD%A6%E8%B5%9B%E9%81%93-Baseline/</url>
      
        <content type="html"><![CDATA[<h2 id="配置阿里云dsw环境">1 配置阿里云DSW环境</h2><p><ahref="https://qwosdw576oc.feishu.cn/docx/NajfdyJm3oripXxrPFFczjSon4z">阿里云机器学习Pai-DSW服务器部署教程</a></p><h2 id="下载数据">2 下载数据</h2><ul><li><ahref="https://help.aliyun.com/zh/oss/developer-reference/install-ossutil?spm=a2c22.12281978.0.0.48a3558ctL6RrI">安装ossutil 工具</a> ossutil 是 OSS 的命令行管理工具，支持Windows、Linux、macOS、ARM 系统。可以通过 ossutil提供的方便、简洁、丰富的 Bucket 和 Object 命令管理自己的 OSS。<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -v ; curl https://gosspublic.alicdn.com/ossutil/install.sh | sudo bash</span><br></pre></td></tr></table></figure></li><li>分别复制训练集和测试集的 ossutil 内网命令至 DSW 的Terminal，即可下载</li></ul><h2 id="解压数据">3 解压数据</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">解压训练集的压缩包文件</span></span><br><span class="line">!unzip ai4bio_trainset.zip</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">解压测试集的压缩包文件</span></span><br><span class="line">!unzip ai4bio_testset_final.zip</span><br></pre></td></tr></table></figure><h2 id="运行-life-baseline.ipynb">4 运行 life-baseline.ipynb</h2><ul><li>将 baseline 代码上传至 DSW</li><li>运行代码</li></ul><h2 id="提交结果">5 提交结果</h2><ul><li>下载 baseline 运行后生成的文件 submit_2.txt</li><li>在比赛界面提交结果文件</li><li>在 我的成绩 界面查看成绩</li></ul>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Git常用命令</title>
      <link href="/2023/08/17/Git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2023/08/17/Git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="使用代理">1 使用代理</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用http代理</span></span><br><span class="line">git config --global http.proxy http://127.0.0.1:7890</span><br><span class="line">git config --global https.proxy https://127.0.0.1:7890</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> remark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Conda常用命令</title>
      <link href="/2023/08/17/Conda%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2023/08/17/Conda%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="查看版本">1 查看版本</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda --version</span><br></pre></td></tr></table></figure><h2 id="虚拟环境">2 虚拟环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda info -e # 显示所有虚拟环境</span><br></pre></td></tr></table></figure><h2 id="使用代理">3 使用代理</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda config --set proxy_servers.http http://127.0.0.1:7890</span><br><span class="line">conda config --set proxy_servers.https https://127.0.0.1:7890</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> remark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令</title>
      <link href="/2023/08/17/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2023/08/17/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="版本">1 版本</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看linux发行版本</span><br><span class="line">lsb_release -a</span><br><span class="line"></span><br><span class="line">查看linux内核版本</span><br><span class="line">uname -a</span><br></pre></td></tr></table></figure><h2 id="显卡">2 显卡</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><h2 id="磁盘">3 磁盘</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df -h</span><br></pre></td></tr></table></figure><h2 id="用户">用户</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /home</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> remark </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>test</title>
      <link href="/2023/08/16/test/"/>
      <url>/2023/08/16/test/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Baz </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Foo </tag>
            
            <tag> Bar </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/08/15/hello-world/"/>
      <url>/2023/08/15/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
