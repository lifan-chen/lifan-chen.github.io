<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Paper: https:&#x2F;&#x2F;openreview.net&#x2F;forum?id&#x3D;yzkSU5zdwD Cite as: Wei J, Tay Y, Bommasani R, et al. Emergent Abilities of Large Language Models[J]. Transactions on Machine Learning Research, 2022.  What Emer">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM的涌现能力">
<meta property="og:url" content="http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/index.html">
<meta property="og:site_name" content="Shimmer">
<meta property="og:description" content="Paper: https:&#x2F;&#x2F;openreview.net&#x2F;forum?id&#x3D;yzkSU5zdwD Cite as: Wei J, Tay Y, Bommasani R, et al. Emergent Abilities of Large Language Models[J]. Transactions on Machine Learning Research, 2022.  What Emer">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/image-20240508190030591.png">
<meta property="article:published_time" content="2024-05-08T07:51:59.000Z">
<meta property="article:modified_time" content="2024-05-08T12:38:53.997Z">
<meta property="article:author" content="Zoe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/image-20240508190030591.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.png">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>LLM的涌现能力</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
    <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/tags/">Tag</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2024/05/09/Scaling-Laws/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2024/05/08/LLM/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&text=LLM的涌现能力"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&title=LLM的涌现能力"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&is_video=false&description=LLM的涌现能力"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=LLM的涌现能力&body=Check out this article: http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&title=LLM的涌现能力"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&title=LLM的涌现能力"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&title=LLM的涌现能力"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&title=LLM的涌现能力"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&name=LLM的涌现能力&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&t=LLM的涌现能力"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#what"><span class="toc-number">1.</span> <span class="toc-text"> What</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#which"><span class="toc-number">2.</span> <span class="toc-text"> Which</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#few-shot-prompted-tasks"><span class="toc-number">2.1.</span> <span class="toc-text"> Few-Shot Prompted Tasks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#augmented-prompting-strategies"><span class="toc-number">2.2.</span> <span class="toc-text"> Augmented Prompting Strategies</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#discussion"><span class="toc-number">3.</span> <span class="toc-text"> Discussion</span></a></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        LLM的涌现能力
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Zoe</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-05-08T07:51:59.000Z" class="dt-published" itemprop="datePublished">2024-05-08</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/Note/">Note</a> › <a class="category-link" href="/categories/Note/AI/">AI</a> › <a class="category-link" href="/categories/Note/AI/NLP/">NLP</a> › <a class="category-link" href="/categories/Note/AI/NLP/LLM/">LLM</a>
    </div>


      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <p>Paper: <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=yzkSU5zdwD">https://openreview.net/forum?id=yzkSU5zdwD</a></p>
<p>Cite as: Wei J, Tay Y, Bommasani R, et al. Emergent Abilities of Large Language Models[J]. Transactions on Machine Learning Research, 2022.</p>
<h2 id="what"><a class="markdownIt-Anchor" href="#what"></a> What</h2>
<p><mark>Emergent ability of large language model</mark>: An ability is emergent if it is not present in smaller models but is present in larger models.</p>
<p>Emergent abilities would not have been directly predicted by extrapolating a scaling law from small-scale models.</p>
<p>When visualized via a scaling curve (x-axis: model scale, y-axis: performance), emergent abilities show a clear pattern: performance is near-random until a certain critical threshold of scale is reached, after which performance increases to substantially above random.</p>
<p>Today’s language models have been scaled primarily along three factors:</p>
<ul>
<li>amount of computation</li>
<li>number of model parameters</li>
<li>training dataset size</li>
</ul>
<p>Overall, it may be wise to view emergence as a function of many correlated variables.</p>
<p>Note that the scale at which an ability is first observed to emerge depends on a number of factors and  is not an immutable property of the ability.</p>
<p>Our goal in this paper is not  to characterize or claim that a specific scale is required to observe emergent abilities, but rather, we aim to discuss examples of emergent behavior in prior work.</p>
<h2 id="which"><a class="markdownIt-Anchor" href="#which"></a> Which</h2>
<h3 id="few-shot-prompted-tasks"><a class="markdownIt-Anchor" href="#few-shot-prompted-tasks"></a> Few-Shot Prompted Tasks</h3>
<p>Emergent abilities in the <em>prompting</em> paradigm, as popularized by GPT-3.</p>
<p>In prompting, a pre-trained language model is given a prompt of a task and completes the response without any further training or gradient updates to its parameters. Brown et al. proposed <em>few-shot prompting</em>, which includes a few input-output examples in the model’s context (input) as a preamble before asking the model to perform the task for an unseen inference-time example.</p>
<p>The ability to perform a task via few-shot prompting is emergent when a model has random performance until a certain scale, after which performance increases to well-above random.</p>
<h3 id="augmented-prompting-strategies"><a class="markdownIt-Anchor" href="#augmented-prompting-strategies"></a> Augmented Prompting Strategies</h3>
<ul>
<li>
<p>Multi-step reasoning</p>
<p>A recent prompting strategy called Chain-of-thought prompting enables language models to solve such problems by guiding them to produce a sequence of intermediate steps before giving the final answer.</p>
<p>A similar emergence in performance gain was also observed when augmenting few-shot prompting with explanations that came after the final answer<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p>
</li>
<li>
<p>Instruction following</p>
<p>Aim to better enable language models to perform new task simply by reading instructions describing the task (without few-shot examples). By finetuning on a mixture of tasks phrased as instructions, language models have been shown to respond appropriately to instructions describing an unseen task.</p>
</li>
<li>
<p>Program execution 程序执行</p>
<p>Nye et al. show that  finetuning language models to predict intermediate outputs (“scratchpad”) enables them to successfully execute such multi-step computations.</p>
</li>
<li>
<p>Model calibration 模型校准</p>
<p>Calibration: measure whether models can predict which questions they will be able to answer correctly.</p>
</li>
</ul>
<p><img src="LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/image-20240508190030591.png" alt="image-20240508190030591" /></p>
<h2 id="discussion"><a class="markdownIt-Anchor" href="#discussion"></a> Discussion</h2>
<ul>
<li>
<p>Potential explanations of emergence</p>
<p>Although there are dozens of examples of emergent abilities, there are currently few compelling explanations for why such abilities emerge in the way they do.</p>
<p>It is also important to consider the evaluation metrics used to measure emergent abilities.</p>
<p>Overall, more work is needed to tease apart what enables scale to unlock emergent abilities.</p>
</li>
<li>
<p>Beyond Scaling</p>
<p>Although we may observe an emergent ability to occur at a certain scale, it is possible that the ability could be later achieved at a smaller scale–in other words, model scale is not the singular factor for unlocking an emergent ability. As the science of training large language models processes, certain abilities may be unlocked for smaller models with new architectures, higher-quality data, or improved training procedures.</p>
<p>Moreover, once an ability is discovered, further research may make the ability available for smaller scale models.</p>
</li>
<li>
<p>Another view of emergence</p>
<p>Scale need not be the only lens to view emergent abilities. Overall, emergent abilities should probably be viewed as a function of many correlated variables.</p>
</li>
<li>
<p>Emergent risks</p>
</li>
<li>
<p>Sociological changes</p>
</li>
<li>
<p>Directions for future work</p>
<ul>
<li>Further model scaling</li>
<li>Improved model architectures and training</li>
<li>Data scaling</li>
<li>Better techniques for and understanding of prompting</li>
<li>Frontier tasks</li>
<li>Understand emergence</li>
</ul>
</li>
</ul>
<hr class="footnotes-sep" />
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Lampinen A, Dasgupta I, Chan S, et al. Can language models learn from explanations in context?[C]//Goldberg Y, Kozareva Z, Zhang Y. Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, 2022: 537-563. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a href="/categories/">Category</a></li>
        
          <li><a href="/tags/">Tag</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#what"><span class="toc-number">1.</span> <span class="toc-text"> What</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#which"><span class="toc-number">2.</span> <span class="toc-text"> Which</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#few-shot-prompted-tasks"><span class="toc-number">2.1.</span> <span class="toc-text"> Few-Shot Prompted Tasks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#augmented-prompting-strategies"><span class="toc-number">2.2.</span> <span class="toc-text"> Augmented Prompting Strategies</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#discussion"><span class="toc-number">3.</span> <span class="toc-text"> Discussion</span></a></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&text=LLM的涌现能力"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&title=LLM的涌现能力"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&is_video=false&description=LLM的涌现能力"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=LLM的涌现能力&body=Check out this article: http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&title=LLM的涌现能力"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&title=LLM的涌现能力"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&title=LLM的涌现能力"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&title=LLM的涌现能力"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&name=LLM的涌现能力&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2024/05/08/LLM%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B/&t=LLM的涌现能力"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2023-present
    Zoe
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/tags/">Tag</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
